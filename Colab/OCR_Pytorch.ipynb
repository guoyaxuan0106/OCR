{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OCR_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9OHEzwXIvmCL",
        "-cpr242KuQc3",
        "HpY-hAJ0uQdB",
        "dbNPzexduQdH",
        "uaQpfg-iuQdL",
        "DyyITROefZtw"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHgFIw1auQcr"
      },
      "source": [
        "import os\n",
        "import fnmatch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.utils.data as Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cpr242KuQc3"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UtuWTp4cy6u"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip '/content/drive/My Drive/OCR/Data/dataset.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtJTiEllx815"
      },
      "source": [
        "# !rm -rf dataset dataset.zip __MACOSX\n",
        "# # small dataset\n",
        "# !wget https://transfer.sh/MrGxw/dataset.zip\n",
        "# # large dataset\n",
        "# #!wget https://transfer.sh/NwLvB/dataset.zip\n",
        "# !unzip -qq dataset.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKVeyaJTGHMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "562b43e2-ea74-4069-9d66-2a9bc32a25fd"
      },
      "source": [
        "# In this project, we train the model to recognize the word by predicting each character/digit for words\n",
        "# So the char_list is the list of all characters (distinguish captical and lower case) and digits\n",
        "# And we transform each word into char vector\n",
        "\n",
        "# char_list:   'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\n",
        "char_list = string.ascii_letters + string.digits\n",
        "print('char_list:',char_list)\n",
        "print('total length:', len(char_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "char_list: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
            "total length: 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyTDbQdOVyJO"
      },
      "source": [
        "# every word is encoded as a list of digits\n",
        "# the digit for each character is represented by the index\n",
        "# e.g. aabb -> [0,0,1,1], index of a is 0, index of b is 1\n",
        "\n",
        "def encode_to_labels(txt):\n",
        "    # encoding each output word into digits\n",
        "    dig_lst = []\n",
        "    for index, char in enumerate(txt):\n",
        "        try:\n",
        "            dig_lst.append(char_list.index(char))\n",
        "        except:\n",
        "            print(char)\n",
        "        \n",
        "    return dig_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av_nziAQkG9a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path = 'dataset/'\n",
        "\n",
        "x = [] # the image\n",
        "y = [] # the char vector\n",
        "x_len = [] #?\n",
        "y_len = [] #?\n",
        "orig_y = [] # the original word\n",
        " \n",
        "max_label_len = 0\n",
        " \n",
        "# why flag\n",
        "flag = 0\n",
        " \n",
        "for i, f_name in enumerate(glob(os.path.join(path,'*/*.jpg'))):\n",
        "\n",
        "    # read input image and convert into gray scale image\n",
        "    img = cv2.cvtColor(cv2.imread(f_name), cv2.COLOR_BGR2GRAY)  \n",
        "\n",
        "    # convert each image of shape (32, 128, 1)\n",
        "    img = cv2.resize(img,(128,32))\n",
        "    img = np.expand_dims(img , axis = 0)\n",
        "\n",
        "    # Normalize each image\n",
        "    img = img/255.\n",
        "\n",
        "    # get the text from the image\n",
        "    txt = os.path.basename(f_name).split('_')[1]\n",
        "\n",
        "    # compute maximum length of the text\n",
        "    if len(txt) > max_label_len:\n",
        "        max_label_len = len(txt)\n",
        "\n",
        "    x.append(img)\n",
        "    y.append(encode_to_labels(txt)) \n",
        "    x_len.append(31)\n",
        "    y_len.append(len(txt))\n",
        "    orig_y.append(str(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwvHPaE4uQc_"
      },
      "source": [
        "# pad each output label to maximum text length\n",
        "# use \"post\" padding\n",
        "# this is not zero padding, we want to pad a specific value: len(char_list) + 1\n",
        " \n",
        "for vector in y:\n",
        "  vector.extend([len(char_list)] * (max_label_len - len(vector)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpY-hAJ0uQdB"
      },
      "source": [
        "# Model Archtecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cQbYU9J30BT",
        "cellView": "code"
      },
      "source": [
        "class CRNN(nn.Module):\n",
        "  def __init__(self, char_list):\n",
        "    super(CRNN, self).__init__()\n",
        "    \n",
        "    # Input with shape of height = 32 and width = 128 \n",
        "\n",
        "    # Conv2D: 64 filters, kernels (3,3), rectified unit\n",
        "    # Pooling: size (2, 2), stride 2\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, stride = 2)\n",
        "    )\n",
        "\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, stride = 2)\n",
        "    )\n",
        "\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d((2,1))\n",
        "    )\n",
        "    \n",
        "    # Batch normalization layer, \n",
        "    # blog: https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c\n",
        "    self.layer5 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512) # number of channels\n",
        "    )\n",
        "\n",
        "    self.layer6 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = 1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.MaxPool2d((2, 1))\n",
        "    )\n",
        "\n",
        "    self.layer7 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size = 512, hidden_size = 128, num_layers = 2, dropout = 0.2, batch_first = True, bidirectional = True)\n",
        "    \n",
        "    self.fc = nn.Linear(in_features = 256, out_features = len(char_list) + 1)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim = 2)\n",
        "\n",
        "  def forward(self, x, device):\n",
        "\n",
        "    # x.shape = (batch_size, channel_size, width, height)\n",
        "\n",
        "    # DCNN\n",
        "    output = self.layer1(x)\n",
        "    output = self.layer2(output)\n",
        "    output = self.layer3(output)\n",
        "    output = self.layer4(output)\n",
        "    output = self.layer5(output)\n",
        "    output = self.layer6(output)\n",
        "    output = self.layer7(output)\n",
        "\n",
        "    # reduce the dimension\n",
        "    output = torch.squeeze(output, 2)\n",
        "    # print(output.shape)\n",
        "\n",
        "    # LSTM: we want to return sequences, not the last output\n",
        "    # here the ouput_size (which is the input for lstm) is (batch_size, num_channels, num_features)\n",
        "    # the required \"input\" for lstem is (seq_len, batch, input_size)\n",
        "    # so we have to reshape the output\n",
        "    output = output.permute(2, 0, 1)\n",
        "    # print(output.shape)\n",
        "\n",
        "    # Initilize h and c\n",
        "    # Their size is (num_layers * num_directions, batch, hidden_size)\n",
        "    h_shape = (4, output.shape[0], 128)\n",
        "    h = Variable(torch.zeros(h_shape)).to(device)\n",
        "    c = Variable(torch.zeros(h_shape)).to(device)\n",
        "\n",
        "    output, (_, _) = self.lstm(output, (h, c))\n",
        "    # print(output.shape)\n",
        "\n",
        "    # our final output has [len(char_list)+1] classes\n",
        "    # we need to use softmax as the activation function\n",
        "    output = self.fc(output)\n",
        "    output = self.softmax(output)\n",
        "    # print(output.shape)\n",
        "\n",
        "    return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaQpfg-iuQdL"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEMxxHZsKdjq"
      },
      "source": [
        "## Training per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRS2xtHsH5nu"
      },
      "source": [
        "def train(train_loader, model, loss_function, optimizer, i, num_epoch, device):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param model: model\n",
        "    :param loss_function: loss layer\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()  # train mode\n",
        "    losses = []\n",
        "\n",
        "    # Load by batches\n",
        "    for (train_x, train_y, train_x_len, train_y_len, train_orig_y) in train_loader:\n",
        "        \n",
        "        train_x = train_x.to(device)\n",
        "        predict_y = model(train_x, device)\n",
        "        # print(torch.sum(predict_y, dim = 2))\n",
        "\n",
        "        # CTC_LOSS: https://zhuanlan.zhihu.com/p/108547594\n",
        "        # ctc_loss shape: https://zhuanlan.zhihu.com/p/67415439\n",
        "        # input: (seq_len, batch_size, length of char list)\n",
        "        # target: (batch_size, len_)\n",
        "        train_x_len = train_x_len.type(torch.LongTensor)\n",
        "        train_y_len = train_y_len.type(torch.LongTensor)\n",
        "\n",
        "        # print(type(predict_y))\n",
        "        # print(type(train_y))\n",
        "\n",
        "        loss = ctc_loss(predict_y, train_y, train_x_len, train_y_len)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # Print status for each epoch \n",
        "    print('Epoch: [{0}/{1}] \\t Train_loss: {2}'.format(i, num_epoch, sum(losses)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUbQKMDwJvwz"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "# Here we change orig_y into labels for putting into torch.Tensor\n",
        "le = preprocessing.LabelEncoder()\n",
        "label_y = le.fit_transform(orig_y)\n",
        "\n",
        "# Use le.inverse_transform(label_y) to transfer back to list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DoRi1T8mDHz"
      },
      "source": [
        "# Transfer all variables into torch tensor\n",
        "x = Variable(torch.Tensor(x))\n",
        "y = Variable(torch.Tensor(y))\n",
        "x_len = Variable(torch.Tensor(x_len))\n",
        "y_len = Variable(torch.Tensor(y_len))\n",
        "label_y = Variable(torch.Tensor(label_y))\n",
        "\n",
        "# Train Test Split\n",
        "train_x, val_x, train_y, val_y, train_x_len, val_x_len, train_y_len, val_y_len, train_label_y, val_label_y = train_test_split(x, y, x_len, y_len, label_y, test_size = 0.1, random_state = 2)\n",
        "\n",
        "# Training and testing dataset\n",
        "train_dataset = Data.TensorDataset(train_x, train_y, train_x_len, train_y_len, train_label_y)\n",
        "test_dataset = Data.TensorDataset(val_x, val_y, val_x_len, val_y_len, val_label_y)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 200\n",
        "train_loader = Data.DataLoader(train_dataset, batch_size, True)\n",
        "test_loader = Data.DataLoader(test_dataset, batch_size, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqOzYAS5Jhp"
      },
      "source": [
        "# print('Number of training data:',len(train_x))\n",
        "# print('Number of validation data:',len(val_x))\n",
        "\n",
        "# plt.imshow(train_x[0][:,:,0], cmap='gray')\n",
        "# print('Label value: ',train_y[0])\n",
        "# print('Raw Label value: ', train_orig_y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYI14t9N5jI4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "73ee759d-f36e-4104-c1c6-8e95a022c32b"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Loss function: CTC Details: (https://theailearner.com/2019/05/29/connectionist-temporal-classificationctc/)\n",
        "ctc_loss = nn.CTCLoss(blank = len(char_list))\n",
        "learning_rate = 0.01\n",
        "\n",
        "model = CRNN(char_list)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "print('Train in {0} samples, validate in {1} samples \\n'.format(len(train_x), len(val_x)))\n",
        "\n",
        "epoch = 10\n",
        "train(train_loader, model, ctc_loss, optimizer, 1, epoch, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train in 2880 samples, validate in 321 samples \n",
            "\n",
            "Epoch: [1/10] \t Train_loss: -57.0730676651001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWgvfAFLH_ov"
      },
      "source": [
        "## Validation per epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUWGxZZ3H_z_"
      },
      "source": [
        "def validate(val_loader, encoder, decoder, loss_function):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param loss_function: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # eval mode (no dropout or batchnorm)\n",
        "    decoder.eval()  \n",
        "    encoder.eval()\n",
        "\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            encoded_imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(encoded_imgs, caps, caplens)\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "\n",
        "            scores, _, _, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader),\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads\n",
        "                references.append(img_captions)\n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRYeN6RDuQdT"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35vew__I7F4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5f2c1aa7-3622-4a2a-cb6e-9b2cc5254bc6"
      },
      "source": [
        "# Download a trained model\n",
        "!rm best_model.hdf5\n",
        "!wget https://transfer.sh/11ClXx/best_model.hdf5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'best_model.hdf5': No such file or directory\n",
            "--2020-10-26 03:28:24--  https://transfer.sh/11ClXx/best_model.hdf5\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZL3d7XEuQdU"
      },
      "source": [
        "\n",
        "# load the saved best model weights\n",
        "act_model.load_weights('best_model.hdf5')\n",
        " \n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(val_x[:10])\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "# see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(\"original_text =  \", val_orig_y[i])\n",
        "    print(\"predicted text = \", end = '')\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zDmeF7ruQdX"
      },
      "source": [
        "'''\n",
        "We want to know the accuracy [the number of correct/total number]\n",
        " 1. if the predicted text == truth label, we count as 1\n",
        " 2. otherwise, we count as 0\n",
        "e.g.\n",
        "If we have a word \"Theword\",\n",
        "then the predicted text:\n",
        "\"Theword\" -> 1\n",
        "\"Theward\" -> 0\n",
        "'''\n",
        "prediction = act_model.predict(val_x)\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        "\n",
        "count = 0\n",
        "#How to get count?\n",
        "print('Accuracy:', count/len(val_orig_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEBe0iZHeGl1"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyyITROefZtw"
      },
      "source": [
        "## Upload a picture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxgWHseBeMCR"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce7q3_3fXLi"
      },
      "source": [
        "## Detection using open source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpc7jgN-ixjT"
      },
      "source": [
        "#Intall libs\n",
        "#https://gitlab.gnome.org/World/OpenPaperwork/pyocr\n",
        "!apt-get install tesseract-ocr\n",
        "!pip install pyocr\n",
        "!pip install wand\n",
        "!pip install pillow\n",
        "!apt-get install libmagickwand-dev\n",
        "!apt-get install freetype imagemagick\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBPRSEyag-CX"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from wand.image import Image\n",
        "from PIL import Image\n",
        "import pyocr\n",
        "import pyocr.builders\n",
        "import io\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH0Ruf0WiTZ9"
      },
      "source": [
        "img_path = ? #where is the image you uploaded\n",
        "image = cv2.imread(img_path,0)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(image,cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goGWzriljPjx"
      },
      "source": [
        "tool = pyocr.get_available_tools()[0]\n",
        "lang = tool.get_available_languages()[0] #english"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfnNpD0heyol"
      },
      "source": [
        "### Detection of words\n",
        "image = cv2.imread(img_path,0)\n",
        "word_boxes = tool.image_to_string(\n",
        "    Image.open(img_path),\n",
        "    lang=lang,\n",
        "    builder=pyocr.builders.WordBoxBuilder()\n",
        ")\n",
        "for box in word_boxes:\n",
        "    cv2.rectangle(image, box.position[0], box.position[1], color=(0,0,255), thickness=2)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image,cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqnJHbPsiXTL"
      },
      "source": [
        "### Detection of lines\n",
        "image = cv2.imread(img_path,0)\n",
        "line_boxes = tool.image_to_string(\n",
        "    Image.open(img_path), lang=lang,\n",
        "    builder=pyocr.builders.LineBoxBuilder()\n",
        ")\n",
        "for box in line_boxes:\n",
        "    cv2.rectangle(image, box.position[0], box.position[1], color=(0,0,255), thickness=2)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image,cmap='gray')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_O1YERfwG2"
      },
      "source": [
        "## Prediction by our OCR model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj2nJVZ9gCNx"
      },
      "source": [
        "'''\n",
        "Now we want to use the OCR model to predict each boxes\n",
        "- We should use word_boxes instead of line_boxes(Why?)\n",
        "- We should convert the structure of word_boxes to be the input of OCR model\n",
        "Requirements:\n",
        "1. Should crop the box from the original image\n",
        "2. The crop should be resized to (128, 32, 1)\n",
        "3. The crop Should be normalized to 0-1\n",
        "'''\n",
        "\n",
        "boxes_x = []\n",
        "for box in word_boxes:\n",
        "  pos = box.position\n",
        "  crop = ?\n",
        "  boxes_x.append(crop)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2fydiEejKcD"
      },
      "source": [
        "# Once you have the input of OCR model ready\n",
        "# Use functions to the predicted result:\n",
        "# 1. act_model.predict\n",
        "# 2. K.get_value(K.ctc_decode(...))\n",
        "\n",
        "# Note: K.get_value returns a list of list of indexes, each index corresponds to the character in char_list\n",
        "# e.g. you should use char_list[index] to recover the characters\n",
        "#      [0, 1, 0, -1] -> 'a','b','a',blank\n",
        "?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOugMYrhj9ti"
      },
      "source": [
        "## Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72UBOeAac7Go"
      },
      "source": [
        "# Now we want to visualize all the boxes and predicted text on the original image\n",
        "# Hint:\n",
        "# use cv2.rectangle to draw box\n",
        "# use cv2.putText to draw text\n",
        "\n",
        "image = cv2.imread(img_path)\n",
        "?\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image,cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRXSKhWiRzSU"
      },
      "source": [
        "## Output raw text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saFJrinSR98P"
      },
      "source": [
        "e.g. The example image should give output:\n",
        "\n",
        "I hope you all learned something\n",
        "\n",
        "from my class. Keep in touch !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oOJYKnzSkyC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}